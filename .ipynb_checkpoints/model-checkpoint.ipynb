{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the language modeling task, any tokens on the future positions should be masked. To produce a probability distribution over output words, the output of the nn.TransformerEncoder model is passed through a linear layer followed by a log-softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PositionalEncoding module injects some information about the relative or absolute position of the tokens in the sequence. The positional encodings have the same dimension as the embeddings so that the two can be summed. Here, we use sine and cosine functions of different frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and batch data\n",
    "### This tutorial uses torchtext to generate Wikitext-2 dataset. The vocab object is built based on the train dataset and is used to numericalize tokens into tensors. Wikitext-2 represents rare tokens as <unk>.\n",
    "\n",
    "### Given a 1-D vector of sequential data, batchify() arranges the data into batch_size columns. If the data does not divide evenly into batch_size columns, then the data is trimmed to fit. For instance, with the alphabet as the data (total length of 26) and batch_size=4, we would divide the alphabet into 4 sequences of length 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab Vocab()\n",
      "train WikiText2 val WikiText2 test WikiText2\n",
      "tensor([[    9,    59,   564,  ..., 11652,  2435,     1],\n",
      "        [ 3849,    12,   300,  ...,    47,    30,  1990],\n",
      "        [ 3869,   315,    19,  ...,    97,  7720,     4],\n",
      "        ...,\n",
      "        [  587,  4011,    59,  ...,     1,  1439, 12313],\n",
      "        [ 4987,    29,     4,  ...,  3165, 17106,  2060],\n",
      "        [    6,     8,     1,  ...,    62,    18,     2]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "   \n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "# train_iter was \"consumed\" by the process of building the vocab,\n",
    "# so we have to create it again\n",
    "\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "# print(f'train {train_iter} val {val_iter} test {test_iter}')\n",
    "\n",
    "\n",
    "#train data is basically just a list of tensors of sentence tensors \n",
    "\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Args:\n",
    "        data: Tensor, shape [N]\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape [N // bsz, bsz]\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)\n",
    "\n",
    "# print(train_data)git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to generate input and target sequence\n",
    "### get_batch() generates a pair of input-target sequences for the transformer model. It subdivides the source data into chunks of length bptt. For the language modeling task, the model needs the following words as Target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape [full_seq_len, batch_size]\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
    "        target has shape [seq_len * batch_size]\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate an instance\n",
    "### The model hyperparameters are defined below. The vocab size is equal to the length of the vocab object.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.2  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model\n",
    "### We use CrossEntropyLoss with the SGD (stochastic gradient descent) optimizer. The learning rate is initially set to 5.0 and follows a StepLR schedule. During training, we use nn.utils.clip_grad_norm_ to prevent gradients from exploding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "def train(model: nn.Module) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "\n",
    "    num_batches = len(train_data) // bptt\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        batch_size = data.size(0)\n",
    "        if batch_size != bptt:  # only on last batch\n",
    "            src_mask = src_mask[:batch_size, :batch_size]\n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(eval_data, i)\n",
    "            batch_size = data.size(0)\n",
    "            if batch_size != bptt:\n",
    "                src_mask = src_mask[:batch_size, :batch_size]\n",
    "            output = model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += batch_size * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(eval_data) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop over epochs. \n",
    "### Save the model if the validation loss is the best weâ€™ve seen so far. Adjust the learning rate after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 27.83 | loss  6.10 | ppl   445.92\n",
      "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch 26.37 | loss  6.00 | ppl   403.11\n",
      "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch 26.35 | loss  5.80 | ppl   329.27\n",
      "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch 26.92 | loss  5.80 | ppl   330.34\n",
      "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch 26.80 | loss  5.75 | ppl   314.14\n",
      "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch 30.66 | loss  5.87 | ppl   354.86\n",
      "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch 27.39 | loss  5.97 | ppl   389.91\n",
      "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch 26.84 | loss  5.98 | ppl   395.21\n",
      "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch 27.20 | loss  5.91 | ppl   367.12\n",
      "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch 26.87 | loss  5.91 | ppl   369.83\n",
      "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch 26.89 | loss  5.79 | ppl   327.97\n",
      "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch 26.86 | loss  5.87 | ppl   355.86\n",
      "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch 26.82 | loss  5.87 | ppl   353.43\n",
      "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch 26.91 | loss  5.80 | ppl   330.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 83.14s | valid loss  5.70 | valid ppl   299.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2928 batches | lr 4.75 | ms/batch 27.06 | loss  5.68 | ppl   292.77\n",
      "| epoch   2 |   400/ 2928 batches | lr 4.75 | ms/batch 26.93 | loss  5.67 | ppl   289.98\n",
      "| epoch   2 |   600/ 2928 batches | lr 4.75 | ms/batch 27.41 | loss  5.47 | ppl   238.04\n",
      "| epoch   2 |   800/ 2928 batches | lr 4.75 | ms/batch 26.97 | loss  5.51 | ppl   247.61\n",
      "| epoch   2 |  1000/ 2928 batches | lr 4.75 | ms/batch 27.02 | loss  5.47 | ppl   236.43\n",
      "| epoch   2 |  1200/ 2928 batches | lr 4.75 | ms/batch 26.90 | loss  5.55 | ppl   256.56\n",
      "| epoch   2 |  1400/ 2928 batches | lr 4.75 | ms/batch 26.99 | loss  5.62 | ppl   275.38\n",
      "| epoch   2 |  1600/ 2928 batches | lr 4.75 | ms/batch 26.97 | loss  5.64 | ppl   282.81\n",
      "| epoch   2 |  1800/ 2928 batches | lr 4.75 | ms/batch 28.54 | loss  5.60 | ppl   269.18\n",
      "| epoch   2 |  2000/ 2928 batches | lr 4.75 | ms/batch 27.84 | loss  5.62 | ppl   277.07\n",
      "| epoch   2 |  2200/ 2928 batches | lr 4.75 | ms/batch 27.76 | loss  5.50 | ppl   245.07\n",
      "| epoch   2 |  2400/ 2928 batches | lr 4.75 | ms/batch 26.96 | loss  5.61 | ppl   273.31\n",
      "| epoch   2 |  2600/ 2928 batches | lr 4.75 | ms/batch 27.03 | loss  5.61 | ppl   273.12\n",
      "| epoch   2 |  2800/ 2928 batches | lr 4.75 | ms/batch 26.96 | loss  5.54 | ppl   255.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 83.23s | valid loss  5.60 | valid ppl   269.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2928 batches | lr 4.51 | ms/batch 27.13 | loss  5.48 | ppl   240.17\n",
      "| epoch   3 |   400/ 2928 batches | lr 4.51 | ms/batch 26.98 | loss  5.49 | ppl   241.82\n",
      "| epoch   3 |   600/ 2928 batches | lr 4.51 | ms/batch 27.07 | loss  5.28 | ppl   197.03\n",
      "| epoch   3 |   800/ 2928 batches | lr 4.51 | ms/batch 27.11 | loss  5.34 | ppl   208.54\n",
      "| epoch   3 |  1000/ 2928 batches | lr 4.51 | ms/batch 27.11 | loss  5.30 | ppl   200.80\n",
      "| epoch   3 |  1200/ 2928 batches | lr 4.51 | ms/batch 27.09 | loss  5.39 | ppl   218.80\n",
      "| epoch   3 |  1400/ 2928 batches | lr 4.51 | ms/batch 27.13 | loss  5.45 | ppl   233.78\n",
      "| epoch   3 |  1600/ 2928 batches | lr 4.51 | ms/batch 27.07 | loss  5.48 | ppl   239.74\n",
      "| epoch   3 |  1800/ 2928 batches | lr 4.51 | ms/batch 27.10 | loss  5.43 | ppl   227.69\n",
      "| epoch   3 |  2000/ 2928 batches | lr 4.51 | ms/batch 27.19 | loss  5.45 | ppl   233.54\n",
      "| epoch   3 |  2200/ 2928 batches | lr 4.51 | ms/batch 27.11 | loss  5.32 | ppl   204.89\n",
      "| epoch   3 |  2400/ 2928 batches | lr 4.51 | ms/batch 27.09 | loss  5.44 | ppl   230.88\n",
      "| epoch   3 |  2600/ 2928 batches | lr 4.51 | ms/batch 27.11 | loss  5.45 | ppl   232.56\n",
      "| epoch   3 |  2800/ 2928 batches | lr 4.51 | ms/batch 27.18 | loss  5.38 | ppl   217.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 82.94s | valid loss  5.55 | valid ppl   257.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 2928 batches | lr 4.29 | ms/batch 27.27 | loss  5.35 | ppl   209.76\n",
      "| epoch   4 |   400/ 2928 batches | lr 4.29 | ms/batch 27.15 | loss  5.36 | ppl   213.16\n",
      "| epoch   4 |   600/ 2928 batches | lr 4.29 | ms/batch 27.15 | loss  5.16 | ppl   174.96\n",
      "| epoch   4 |   800/ 2928 batches | lr 4.29 | ms/batch 27.15 | loss  5.22 | ppl   184.55\n",
      "| epoch   4 |  1000/ 2928 batches | lr 4.29 | ms/batch 27.12 | loss  5.18 | ppl   176.98\n",
      "| epoch   4 |  1200/ 2928 batches | lr 4.29 | ms/batch 27.13 | loss  5.26 | ppl   191.67\n",
      "| epoch   4 |  1400/ 2928 batches | lr 4.29 | ms/batch 27.06 | loss  5.32 | ppl   203.93\n",
      "| epoch   4 |  1600/ 2928 batches | lr 4.29 | ms/batch 27.09 | loss  5.36 | ppl   211.75\n",
      "| epoch   4 |  1800/ 2928 batches | lr 4.29 | ms/batch 27.15 | loss  5.31 | ppl   201.71\n",
      "| epoch   4 |  2000/ 2928 batches | lr 4.29 | ms/batch 27.12 | loss  5.32 | ppl   205.11\n",
      "| epoch   4 |  2200/ 2928 batches | lr 4.29 | ms/batch 28.63 | loss  5.19 | ppl   178.88\n",
      "| epoch   4 |  2400/ 2928 batches | lr 4.29 | ms/batch 27.32 | loss  5.30 | ppl   200.48\n",
      "| epoch   4 |  2600/ 2928 batches | lr 4.29 | ms/batch 27.17 | loss  5.32 | ppl   203.91\n",
      "| epoch   4 |  2800/ 2928 batches | lr 4.29 | ms/batch 27.23 | loss  5.25 | ppl   190.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 83.38s | valid loss  5.53 | valid ppl   252.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 2928 batches | lr 4.07 | ms/batch 27.32 | loss  5.24 | ppl   187.89\n",
      "| epoch   5 |   400/ 2928 batches | lr 4.07 | ms/batch 27.27 | loss  5.25 | ppl   190.33\n",
      "| epoch   5 |   600/ 2928 batches | lr 4.07 | ms/batch 27.25 | loss  5.05 | ppl   155.80\n",
      "| epoch   5 |   800/ 2928 batches | lr 4.07 | ms/batch 27.41 | loss  5.11 | ppl   165.87\n",
      "| epoch   5 |  1000/ 2928 batches | lr 4.07 | ms/batch 28.04 | loss  5.07 | ppl   159.20\n",
      "| epoch   5 |  1200/ 2928 batches | lr 4.07 | ms/batch 27.33 | loss  5.15 | ppl   172.76\n",
      "| epoch   5 |  1400/ 2928 batches | lr 4.07 | ms/batch 27.23 | loss  5.21 | ppl   182.47\n",
      "| epoch   5 |  1600/ 2928 batches | lr 4.07 | ms/batch 27.29 | loss  5.25 | ppl   190.56\n",
      "| epoch   5 |  1800/ 2928 batches | lr 4.07 | ms/batch 27.28 | loss  5.19 | ppl   180.27\n",
      "| epoch   5 |  2000/ 2928 batches | lr 4.07 | ms/batch 27.24 | loss  5.21 | ppl   182.91\n",
      "| epoch   5 |  2200/ 2928 batches | lr 4.07 | ms/batch 27.27 | loss  5.08 | ppl   160.24\n",
      "| epoch   5 |  2400/ 2928 batches | lr 4.07 | ms/batch 27.23 | loss  5.20 | ppl   180.77\n",
      "| epoch   5 |  2600/ 2928 batches | lr 4.07 | ms/batch 27.30 | loss  5.21 | ppl   184.01\n",
      "| epoch   5 |  2800/ 2928 batches | lr 4.07 | ms/batch 27.27 | loss  5.15 | ppl   173.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 83.57s | valid loss  5.52 | valid ppl   250.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/ 2928 batches | lr 3.87 | ms/batch 27.45 | loss  5.13 | ppl   168.98\n",
      "| epoch   6 |   400/ 2928 batches | lr 3.87 | ms/batch 27.27 | loss  5.15 | ppl   172.53\n",
      "| epoch   6 |   600/ 2928 batches | lr 3.87 | ms/batch 27.36 | loss  4.96 | ppl   142.50\n",
      "| epoch   6 |   800/ 2928 batches | lr 3.87 | ms/batch 27.34 | loss  5.02 | ppl   152.06\n",
      "| epoch   6 |  1000/ 2928 batches | lr 3.87 | ms/batch 27.24 | loss  4.99 | ppl   146.83\n",
      "| epoch   6 |  1200/ 2928 batches | lr 3.87 | ms/batch 27.28 | loss  5.06 | ppl   157.96\n",
      "| epoch   6 |  1400/ 2928 batches | lr 3.87 | ms/batch 27.26 | loss  5.11 | ppl   165.80\n",
      "| epoch   6 |  1600/ 2928 batches | lr 3.87 | ms/batch 27.26 | loss  5.16 | ppl   173.51\n",
      "| epoch   6 |  1800/ 2928 batches | lr 3.87 | ms/batch 27.32 | loss  5.11 | ppl   165.09\n",
      "| epoch   6 |  2000/ 2928 batches | lr 3.87 | ms/batch 27.27 | loss  5.12 | ppl   167.72\n",
      "| epoch   6 |  2200/ 2928 batches | lr 3.87 | ms/batch 27.29 | loss  4.98 | ppl   146.16\n",
      "| epoch   6 |  2400/ 2928 batches | lr 3.87 | ms/batch 27.29 | loss  5.10 | ppl   164.73\n",
      "| epoch   6 |  2600/ 2928 batches | lr 3.87 | ms/batch 27.32 | loss  5.12 | ppl   167.60\n",
      "| epoch   6 |  2800/ 2928 batches | lr 3.87 | ms/batch 27.31 | loss  5.06 | ppl   157.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 83.50s | valid loss  5.52 | valid ppl   250.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/ 2928 batches | lr 3.68 | ms/batch 27.43 | loss  5.05 | ppl   156.18\n",
      "| epoch   7 |   400/ 2928 batches | lr 3.68 | ms/batch 27.42 | loss  5.07 | ppl   159.43\n",
      "| epoch   7 |   600/ 2928 batches | lr 3.68 | ms/batch 27.31 | loss  4.88 | ppl   131.22\n",
      "| epoch   7 |   800/ 2928 batches | lr 3.68 | ms/batch 27.33 | loss  4.94 | ppl   140.04\n",
      "| epoch   7 |  1000/ 2928 batches | lr 3.68 | ms/batch 27.33 | loss  4.92 | ppl   136.68\n",
      "| epoch   7 |  1200/ 2928 batches | lr 3.68 | ms/batch 27.34 | loss  4.97 | ppl   144.72\n",
      "| epoch   7 |  1400/ 2928 batches | lr 3.68 | ms/batch 27.33 | loss  5.03 | ppl   152.21\n",
      "| epoch   7 |  1600/ 2928 batches | lr 3.68 | ms/batch 27.39 | loss  5.07 | ppl   159.46\n",
      "| epoch   7 |  1800/ 2928 batches | lr 3.68 | ms/batch 27.34 | loss  5.03 | ppl   152.43\n",
      "| epoch   7 |  2000/ 2928 batches | lr 3.68 | ms/batch 27.33 | loss  5.04 | ppl   153.84\n",
      "| epoch   7 |  2200/ 2928 batches | lr 3.68 | ms/batch 27.28 | loss  4.90 | ppl   134.10\n",
      "| epoch   7 |  2400/ 2928 batches | lr 3.68 | ms/batch 27.28 | loss  5.02 | ppl   151.39\n",
      "| epoch   7 |  2600/ 2928 batches | lr 3.68 | ms/batch 27.37 | loss  5.03 | ppl   153.48\n",
      "| epoch   7 |  2800/ 2928 batches | lr 3.68 | ms/batch 27.63 | loss  4.97 | ppl   144.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 83.69s | valid loss  5.49 | valid ppl   243.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/ 2928 batches | lr 3.49 | ms/batch 27.46 | loss  4.98 | ppl   145.13\n",
      "| epoch   8 |   400/ 2928 batches | lr 3.49 | ms/batch 27.32 | loss  4.99 | ppl   147.31\n",
      "| epoch   8 |   600/ 2928 batches | lr 3.49 | ms/batch 27.37 | loss  4.81 | ppl   122.25\n",
      "| epoch   8 |   800/ 2928 batches | lr 3.49 | ms/batch 27.35 | loss  4.86 | ppl   129.25\n",
      "| epoch   8 |  1000/ 2928 batches | lr 3.49 | ms/batch 27.38 | loss  4.85 | ppl   127.14\n",
      "| epoch   8 |  1200/ 2928 batches | lr 3.49 | ms/batch 27.39 | loss  4.91 | ppl   135.09\n",
      "| epoch   8 |  1400/ 2928 batches | lr 3.49 | ms/batch 27.41 | loss  4.95 | ppl   140.99\n",
      "| epoch   8 |  1600/ 2928 batches | lr 3.49 | ms/batch 27.14 | loss  4.99 | ppl   147.44\n",
      "| epoch   8 |  1800/ 2928 batches | lr 3.49 | ms/batch 27.20 | loss  4.95 | ppl   141.40\n",
      "| epoch   8 |  2000/ 2928 batches | lr 3.49 | ms/batch 27.19 | loss  4.96 | ppl   142.90\n",
      "| epoch   8 |  2200/ 2928 batches | lr 3.49 | ms/batch 27.25 | loss  4.82 | ppl   124.29\n",
      "| epoch   8 |  2400/ 2928 batches | lr 3.49 | ms/batch 27.19 | loss  4.95 | ppl   140.62\n",
      "| epoch   8 |  2600/ 2928 batches | lr 3.49 | ms/batch 27.16 | loss  4.96 | ppl   142.51\n",
      "| epoch   8 |  2800/ 2928 batches | lr 3.49 | ms/batch 27.16 | loss  4.89 | ppl   132.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 83.31s | valid loss  5.52 | valid ppl   250.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/ 2928 batches | lr 3.32 | ms/batch 27.35 | loss  4.90 | ppl   134.34\n",
      "| epoch   9 |   400/ 2928 batches | lr 3.32 | ms/batch 27.17 | loss  4.91 | ppl   136.07\n",
      "| epoch   9 |   600/ 2928 batches | lr 3.32 | ms/batch 27.20 | loss  4.74 | ppl   114.53\n",
      "| epoch   9 |   800/ 2928 batches | lr 3.32 | ms/batch 27.17 | loss  4.80 | ppl   122.00\n",
      "| epoch   9 |  1000/ 2928 batches | lr 3.32 | ms/batch 27.20 | loss  4.78 | ppl   118.91\n",
      "| epoch   9 |  1200/ 2928 batches | lr 3.32 | ms/batch 27.32 | loss  4.84 | ppl   126.24\n",
      "| epoch   9 |  1400/ 2928 batches | lr 3.32 | ms/batch 27.24 | loss  4.88 | ppl   131.60\n",
      "| epoch   9 |  1600/ 2928 batches | lr 3.32 | ms/batch 27.15 | loss  4.93 | ppl   138.15\n",
      "| epoch   9 |  1800/ 2928 batches | lr 3.32 | ms/batch 27.20 | loss  4.88 | ppl   132.08\n",
      "| epoch   9 |  2000/ 2928 batches | lr 3.32 | ms/batch 27.24 | loss  4.89 | ppl   133.48\n",
      "| epoch   9 |  2200/ 2928 batches | lr 3.32 | ms/batch 27.21 | loss  4.76 | ppl   116.41\n",
      "| epoch   9 |  2400/ 2928 batches | lr 3.32 | ms/batch 27.25 | loss  4.87 | ppl   130.88\n",
      "| epoch   9 |  2600/ 2928 batches | lr 3.32 | ms/batch 27.23 | loss  4.89 | ppl   133.10\n",
      "| epoch   9 |  2800/ 2928 batches | lr 3.32 | ms/batch 27.20 | loss  4.83 | ppl   125.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 83.15s | valid loss  5.50 | valid ppl   245.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/ 2928 batches | lr 3.15 | ms/batch 27.29 | loss  4.84 | ppl   126.22\n",
      "| epoch  10 |   400/ 2928 batches | lr 3.15 | ms/batch 27.13 | loss  4.86 | ppl   129.09\n",
      "| epoch  10 |   600/ 2928 batches | lr 3.15 | ms/batch 27.16 | loss  4.68 | ppl   107.71\n",
      "| epoch  10 |   800/ 2928 batches | lr 3.15 | ms/batch 27.25 | loss  4.75 | ppl   115.51\n",
      "| epoch  10 |  1000/ 2928 batches | lr 3.15 | ms/batch 27.20 | loss  4.72 | ppl   112.38\n",
      "| epoch  10 |  1200/ 2928 batches | lr 3.15 | ms/batch 27.22 | loss  4.78 | ppl   119.37\n",
      "| epoch  10 |  1400/ 2928 batches | lr 3.15 | ms/batch 27.21 | loss  4.82 | ppl   123.43\n",
      "| epoch  10 |  1600/ 2928 batches | lr 3.15 | ms/batch 27.22 | loss  4.86 | ppl   129.06\n",
      "| epoch  10 |  1800/ 2928 batches | lr 3.15 | ms/batch 27.22 | loss  4.82 | ppl   124.25\n",
      "| epoch  10 |  2000/ 2928 batches | lr 3.15 | ms/batch 27.21 | loss  4.83 | ppl   125.31\n",
      "| epoch  10 |  2200/ 2928 batches | lr 3.15 | ms/batch 27.62 | loss  4.69 | ppl   109.17\n",
      "| epoch  10 |  2400/ 2928 batches | lr 3.15 | ms/batch 27.23 | loss  4.81 | ppl   122.40\n",
      "| epoch  10 |  2600/ 2928 batches | lr 3.15 | ms/batch 27.21 | loss  4.82 | ppl   124.16\n",
      "| epoch  10 |  2800/ 2928 batches | lr 3.15 | ms/batch 27.25 | loss  4.77 | ppl   118.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 83.18s | valid loss  5.53 | valid ppl   250.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   200/ 2928 batches | lr 2.99 | ms/batch 27.35 | loss  4.78 | ppl   118.51\n",
      "| epoch  11 |   400/ 2928 batches | lr 2.99 | ms/batch 27.28 | loss  4.80 | ppl   120.99\n",
      "| epoch  11 |   600/ 2928 batches | lr 2.99 | ms/batch 27.22 | loss  4.62 | ppl   101.33\n",
      "| epoch  11 |   800/ 2928 batches | lr 2.99 | ms/batch 27.25 | loss  4.69 | ppl   108.43\n",
      "| epoch  11 |  1000/ 2928 batches | lr 2.99 | ms/batch 27.21 | loss  4.67 | ppl   106.59\n",
      "| epoch  11 |  1200/ 2928 batches | lr 2.99 | ms/batch 27.22 | loss  4.73 | ppl   112.95\n",
      "| epoch  11 |  1400/ 2928 batches | lr 2.99 | ms/batch 27.26 | loss  4.75 | ppl   116.08\n",
      "| epoch  11 |  1600/ 2928 batches | lr 2.99 | ms/batch 27.16 | loss  4.81 | ppl   122.55\n",
      "| epoch  11 |  1800/ 2928 batches | lr 2.99 | ms/batch 27.18 | loss  4.77 | ppl   117.78\n",
      "| epoch  11 |  2000/ 2928 batches | lr 2.99 | ms/batch 27.26 | loss  4.77 | ppl   117.41\n",
      "| epoch  11 |  2200/ 2928 batches | lr 2.99 | ms/batch 27.24 | loss  4.63 | ppl   102.25\n",
      "| epoch  11 |  2400/ 2928 batches | lr 2.99 | ms/batch 27.20 | loss  4.75 | ppl   115.54\n",
      "| epoch  11 |  2600/ 2928 batches | lr 2.99 | ms/batch 27.22 | loss  4.76 | ppl   116.82\n",
      "| epoch  11 |  2800/ 2928 batches | lr 2.99 | ms/batch 27.25 | loss  4.71 | ppl   111.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 83.18s | valid loss  5.54 | valid ppl   255.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   200/ 2928 batches | lr 2.84 | ms/batch 27.43 | loss  4.72 | ppl   112.46\n",
      "| epoch  12 |   400/ 2928 batches | lr 2.84 | ms/batch 27.23 | loss  4.75 | ppl   115.26\n",
      "| epoch  12 |   600/ 2928 batches | lr 2.84 | ms/batch 27.25 | loss  4.57 | ppl    96.91\n",
      "| epoch  12 |   800/ 2928 batches | lr 2.84 | ms/batch 27.28 | loss  4.64 | ppl   103.05\n",
      "| epoch  12 |  1000/ 2928 batches | lr 2.84 | ms/batch 27.25 | loss  4.62 | ppl   101.56\n",
      "| epoch  12 |  1200/ 2928 batches | lr 2.84 | ms/batch 27.27 | loss  4.67 | ppl   107.07\n",
      "| epoch  12 |  1400/ 2928 batches | lr 2.84 | ms/batch 27.21 | loss  4.70 | ppl   110.29\n",
      "| epoch  12 |  1600/ 2928 batches | lr 2.84 | ms/batch 27.34 | loss  4.75 | ppl   115.85\n",
      "| epoch  12 |  1800/ 2928 batches | lr 2.84 | ms/batch 27.24 | loss  4.71 | ppl   111.56\n",
      "| epoch  12 |  2000/ 2928 batches | lr 2.84 | ms/batch 27.23 | loss  4.72 | ppl   111.77\n",
      "| epoch  12 |  2200/ 2928 batches | lr 2.84 | ms/batch 27.29 | loss  4.57 | ppl    97.02\n",
      "| epoch  12 |  2400/ 2928 batches | lr 2.84 | ms/batch 27.31 | loss  4.69 | ppl   109.28\n",
      "| epoch  12 |  2600/ 2928 batches | lr 2.84 | ms/batch 27.20 | loss  4.71 | ppl   111.07\n",
      "| epoch  12 |  2800/ 2928 batches | lr 2.84 | ms/batch 27.25 | loss  4.65 | ppl   104.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 83.28s | valid loss  5.52 | valid ppl   248.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   200/ 2928 batches | lr 2.70 | ms/batch 27.47 | loss  4.67 | ppl   107.18\n",
      "| epoch  13 |   400/ 2928 batches | lr 2.70 | ms/batch 27.26 | loss  4.70 | ppl   109.50\n",
      "| epoch  13 |   600/ 2928 batches | lr 2.70 | ms/batch 27.26 | loss  4.52 | ppl    92.07\n",
      "| epoch  13 |   800/ 2928 batches | lr 2.70 | ms/batch 27.30 | loss  4.59 | ppl    98.30\n",
      "| epoch  13 |  1000/ 2928 batches | lr 2.70 | ms/batch 27.26 | loss  4.57 | ppl    97.00\n",
      "| epoch  13 |  1200/ 2928 batches | lr 2.70 | ms/batch 27.33 | loss  4.63 | ppl   102.39\n",
      "| epoch  13 |  1400/ 2928 batches | lr 2.70 | ms/batch 27.31 | loss  4.65 | ppl   104.82\n",
      "| epoch  13 |  1600/ 2928 batches | lr 2.70 | ms/batch 27.29 | loss  4.70 | ppl   109.92\n",
      "| epoch  13 |  1800/ 2928 batches | lr 2.70 | ms/batch 27.26 | loss  4.67 | ppl   106.40\n",
      "| epoch  13 |  2000/ 2928 batches | lr 2.70 | ms/batch 27.25 | loss  4.67 | ppl   106.23\n",
      "| epoch  13 |  2200/ 2928 batches | lr 2.70 | ms/batch 27.30 | loss  4.53 | ppl    92.32\n",
      "| epoch  13 |  2400/ 2928 batches | lr 2.70 | ms/batch 27.31 | loss  4.64 | ppl   103.93\n",
      "| epoch  13 |  2600/ 2928 batches | lr 2.70 | ms/batch 27.25 | loss  4.66 | ppl   105.19\n",
      "| epoch  13 |  2800/ 2928 batches | lr 2.70 | ms/batch 27.29 | loss  4.61 | ppl   100.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 83.40s | valid loss  5.53 | valid ppl   251.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   200/ 2928 batches | lr 2.57 | ms/batch 27.44 | loss  4.62 | ppl   101.82\n",
      "| epoch  14 |   400/ 2928 batches | lr 2.57 | ms/batch 27.36 | loss  4.65 | ppl   104.21\n",
      "| epoch  14 |   600/ 2928 batches | lr 2.57 | ms/batch 27.35 | loss  4.49 | ppl    88.75\n",
      "| epoch  14 |   800/ 2928 batches | lr 2.57 | ms/batch 27.32 | loss  4.54 | ppl    93.86\n",
      "| epoch  14 |  1000/ 2928 batches | lr 2.57 | ms/batch 27.35 | loss  4.54 | ppl    93.36\n",
      "| epoch  14 |  1200/ 2928 batches | lr 2.57 | ms/batch 27.25 | loss  4.59 | ppl    98.17\n",
      "| epoch  14 |  1400/ 2928 batches | lr 2.57 | ms/batch 27.24 | loss  4.60 | ppl    99.91\n",
      "| epoch  14 |  1600/ 2928 batches | lr 2.57 | ms/batch 27.34 | loss  4.66 | ppl   105.16\n",
      "| epoch  14 |  1800/ 2928 batches | lr 2.57 | ms/batch 27.27 | loss  4.62 | ppl   101.89\n",
      "| epoch  14 |  2000/ 2928 batches | lr 2.57 | ms/batch 27.31 | loss  4.62 | ppl   102.00\n",
      "| epoch  14 |  2200/ 2928 batches | lr 2.57 | ms/batch 27.40 | loss  4.49 | ppl    88.75\n",
      "| epoch  14 |  2400/ 2928 batches | lr 2.57 | ms/batch 27.30 | loss  4.59 | ppl    98.98\n",
      "| epoch  14 |  2600/ 2928 batches | lr 2.57 | ms/batch 27.33 | loss  4.62 | ppl   101.06\n",
      "| epoch  14 |  2800/ 2928 batches | lr 2.57 | ms/batch 27.27 | loss  4.56 | ppl    95.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 83.43s | valid loss  5.53 | valid ppl   252.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   200/ 2928 batches | lr 2.44 | ms/batch 27.46 | loss  4.58 | ppl    97.69\n",
      "| epoch  15 |   400/ 2928 batches | lr 2.44 | ms/batch 27.31 | loss  4.60 | ppl    99.97\n",
      "| epoch  15 |   600/ 2928 batches | lr 2.44 | ms/batch 27.29 | loss  4.44 | ppl    84.94\n",
      "| epoch  15 |   800/ 2928 batches | lr 2.44 | ms/batch 27.32 | loss  4.50 | ppl    90.14\n",
      "| epoch  15 |  1000/ 2928 batches | lr 2.44 | ms/batch 27.30 | loss  4.49 | ppl    89.46\n",
      "| epoch  15 |  1200/ 2928 batches | lr 2.44 | ms/batch 27.31 | loss  4.54 | ppl    93.73\n",
      "| epoch  15 |  1400/ 2928 batches | lr 2.44 | ms/batch 27.32 | loss  4.57 | ppl    96.07\n",
      "| epoch  15 |  1600/ 2928 batches | lr 2.44 | ms/batch 27.35 | loss  4.61 | ppl   100.28\n",
      "| epoch  15 |  1800/ 2928 batches | lr 2.44 | ms/batch 27.33 | loss  4.57 | ppl    97.01\n",
      "| epoch  15 |  2000/ 2928 batches | lr 2.44 | ms/batch 27.35 | loss  4.58 | ppl    97.54\n",
      "| epoch  15 |  2200/ 2928 batches | lr 2.44 | ms/batch 27.30 | loss  4.44 | ppl    84.49\n",
      "| epoch  15 |  2400/ 2928 batches | lr 2.44 | ms/batch 27.34 | loss  4.55 | ppl    94.56\n",
      "| epoch  15 |  2600/ 2928 batches | lr 2.44 | ms/batch 27.32 | loss  4.57 | ppl    96.09\n",
      "| epoch  15 |  2800/ 2928 batches | lr 2.44 | ms/batch 28.28 | loss  4.52 | ppl    91.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 83.88s | valid loss  5.51 | valid ppl   248.23\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 3\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model)\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the best model on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  5.41 | test ppl   223.60\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = evaluate(best_model, test_data)\n",
    "test_ppl = math.exp(test_loss)\n",
    "print('=' * 89)\n",
    "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
    "      f'test ppl {test_ppl:8.2f}')\n",
    "print('=' * 89)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f34dedcb37ed6ce27db27ccfae8de649dcd6fb60e2ab1fa1e87eae91e4095f2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
